---
title: "Case Study 1"
output: html_document
date: "2022-10-10"
---

# Getting Started

> The goal in the case study is to propose a *regression model for predicting the number of practicing physicians* by county using information from the years 1990 and 1992. The data set provides selected county demographic information (CDI) for 440 of the most populous counties in the United States. Each line of data set has an identification number with a county name and state abbreviation and provides information on 14 variables for a single county. The variables are (in the order they are recorded in the `.txt` file)

+-----------------+-----------------------------------+----------------------------------------------------------------------------------------------------+
| Variable Number | Variable Name                     | Variable Description                                                                               |
+=================+===================================+====================================================================================================+
| 1               | Identification Number             | 1-440\                                                                                             |
+-----------------+-----------------------------------+----------------------------------------------------------------------------------------------------+
| 2               | County                            | County Name                                                                                        |
+-----------------+-----------------------------------+----------------------------------------------------------------------------------------------------+
| 3               | State                             | Two-letter state abbreviation                                                                      |
+-----------------+-----------------------------------+----------------------------------------------------------------------------------------------------+
| 4               | Land Area                         | Land area (square miles)                                                                           |
+-----------------+-----------------------------------+----------------------------------------------------------------------------------------------------+
| 5               | Total Population                  | Estimated 1990 population                                                                          |
+-----------------+-----------------------------------+----------------------------------------------------------------------------------------------------+
| 6               | Percent of Population ages 18-24  | Percent of 1990 CDI population                                                                     |
+-----------------+-----------------------------------+----------------------------------------------------------------------------------------------------+
| 7               | Percent of Population 65 or older | Percent of 1990 CDI population ages 65 or older                                                    |
+-----------------+-----------------------------------+----------------------------------------------------------------------------------------------------+
| 8               | Number of Active Physicians       | Number of professional active non-federal physicians during 1990                                   |
+-----------------+-----------------------------------+----------------------------------------------------------------------------------------------------+
| 9               | Number of Hospital Beds           | Total number of beds, cribs, and bassinets during 1990                                             |
+-----------------+-----------------------------------+----------------------------------------------------------------------------------------------------+
| 10              | Total Serious Crimes              | Total number of serious crimes in 1990 as reported by law enforcement agencies                     |
+-----------------+-----------------------------------+----------------------------------------------------------------------------------------------------+
| 11              | Percent High School Graduates     | Percent of adult population who completed 12 or more years of school                               |
+-----------------+-----------------------------------+----------------------------------------------------------------------------------------------------+
| 12              | Percent of Bachelor's Degrees     | Percent of adult population with bachelor's degrees                                                |
+-----------------+-----------------------------------+----------------------------------------------------------------------------------------------------+
| 13              | Percent Below Poverty Level       | Percent of 1990 CDI population with income below poverty level                                     |
+-----------------+-----------------------------------+----------------------------------------------------------------------------------------------------+
| 14              | Percent Unemployment              | Percent of 1990 CDI labor force that is unemployed                                                 |
+-----------------+-----------------------------------+----------------------------------------------------------------------------------------------------+
| 15              | Per Capita Income                 | Per capita income of 1990 CDI population (dollars)                                                 |
+-----------------+-----------------------------------+----------------------------------------------------------------------------------------------------+
| 16              | Total Personal Income             | Total person income of 1990 CDI population (in millions of dollars)                                |
+-----------------+-----------------------------------+----------------------------------------------------------------------------------------------------+
| 17              | Geographic Region                 | Geographic region classification that is used in the US Bureau of the Census: 1=NE, 2=NC, 3=S, 4=W |
+-----------------+-----------------------------------+----------------------------------------------------------------------------------------------------+

## Setting up Data

Importing our data in R

```{r}
data = read.table("data.txt")
head(data)
```

Removing the **ID column** and the categorical **state** and **county** columns

```{r}
data = data[,-c(1, 2, 3)]
head(data)
```

## Exploring the Data

> Creating Graphs to Determine our Next Step

```{r}
hist(data$V8,main="Histogram of V8", breaks=250, col=2)
```

```{r}
hist(data$V8,breaks=250, main="Histogram of V8 < 5000", xlim=range(10, 5000), col=2)
```

```{r}
library(ggplot2)
ggplot(data, aes(V8)) +
  stat_ecdf(geom = "step", pad=FALSE, color=2) +
  ggtitle("Empircal Distribution of V8")
```

Summary Stats for Each Column

```{r}
summary(data[,1:5])
summary(data[,6:10])
summary(data[,11:12])
```

Creating a Correlation Matrix to figure out which columns are highly correlated with other columns to determine what we need to remove.

```{r}
library(corrplot)
M = cor(data[,-c(1, 2, 3)])
corrplot(M, tl.srt = 45)
```

## Scaling the Data

> Dividing the columns that are not currently percentages by the total population column, which allows us to represent the columns as the percent of the population.

```{r}
data_scaled = data
data_scaled$V9 = data_scaled$V9 / data_scaled$V5
data_scaled$V10 = data_scaled$V10 / data_scaled$V5
```

```{R}
round(cor(data_scaled[,-8]), dig=2)
```

```{r}
M = cor(data_scaled[,-8])
corrplot(M)
```

V5 looks highly correlated with V16, so we decide to drop it to reveal any dependencies.

```{r}
anova(lm(V8~.,data_scaled), lm(V8~.-V5, data_scaled))
```

```{r}
data_scaled = data_scaled[colnames(data_scaled) != "V5"]
```

# MLR Time

> Let's start with a full model

```{r}
model.full = lm(V8 ~ ., data_scaled)
summary(model.full)
```

```{r}
plot(model.full)
```

Let's filter out predictors that have high p-values

Permutation Test to determine if we can remove these columns

```{r}
### Permutation Test Function ###
permutation_test = function(using, to_remove, dataset) {
  n.iter = 1000
  fstats = numeric(n.iter)
  predictors = union(using, to_remove)
  full_model = lm(reformulate(predictors, "V8"), data=dataset)
  for (i in 1:n.iter) {
    newdata = dataset;
    newdata[,to_remove] = newdata[sample(nrow(newdata)), to_remove]
    model = lm(reformulate(predictors, "V8"), data=newdata)
    fstats[i] = summary(model)$fstat[1]
  }
  return (length(fstats[fstats > summary(full_model)$fstat[1]])/n.iter)
}

preds = colnames(data_scaled)
preds = preds[preds != 'V8']
permutation_test(preds, c("V4","V6", "V7", "V10", "V13", "V14", "V17"), data_scaled)
```

> we can remove these columns because the p-value is large.

```{r}
permutation_test(preds, c("V9"), data)
```

Backward Selection Time :)

```{r}
### Backward Selection Algorithm :) ###
data_to_use = data_scaled
predictors = names(data_to_use)
predictors = predictors[predictors != "V8"]
all_preds = predictors
model_full = lm(reformulate(predictors, "V8"), data=data_to_use)

model_curr = model_full
for (i in 1:(ncol(data_scaled) - 1)) { 
  p_vals = summary(model_curr)$coef[-1,'Pr(>|t|)']
  lowest_predictor = names(p_vals[which.max(p_vals)])
  # checking p value
  if (p_vals[lowest_predictor] < .05) { break }
  
  reduced_model = lm(reformulate(predictors[predictors != lowest_predictor], "V8"), 
                     data=data_to_use)
  # anova against full model (is adequate replacement)
  if (anova(model_full, reduced_model)$"Pr(>F)"[2] < .05){ break }
  
  # Double checking with permutation test (in case not normal)
  predictors_ = predictors[predictors != lowest_predictor]
  p = permutation_test(all_preds, all_preds[!(all_preds %in% predictors_) ], data_to_use)
  if (p < .05) { break }
  
  predictors = predictors_
  model_curr = reduced_model
}

summary(model_curr)
```

Double checking our work using an ANOVA table.

```{r}
anova(lm(V8~., data_scaled), lm(V8 ~ V9 + V12 + V13 + V16, data_scaled))
```

Large p-value, means we got the desired result.

Testing to see if we can combine columns V12 and V13

```{r}
anova(lm(V8~., data_scaled), lm(V8 ~ V9 + I(V12 + V13) + V16, data_scaled))
```

Since the p-value is large, we are able to combine those columns and create a new model with the columns combined.

```{r}
model = lm(V8 ~ V9 + I(V12 + V13) + V16, data_scaled)
summary(model)
```

**We also consistently have high R squared values.**

# Model Assumptions

## Constant Variance Assumption

```{r}
plot(model, which=1)
```

```{r}
library(lmtest)
bptest(model)
```

The graph and the BP test do not give us our desired results, so we decide to check if transformations fix our issues.

Square Root Transform:

```{r}
sqrt_transform = lm(sqrt(V8) ~ V9 + I(V12 + V13) + V16, data=data_scaled)
plot(sqrt_transform, which=1)
```

```{r}
bptest(sqrt_transform)
```

Square root does not work.

Log Transform:

```{r}
log_transform = lm(log(V8) ~ V9 + I(V12 + V13) + V16, data=data_scaled)
plot(log_transform, which=1)
```

```{r}
bptest(log_transform)
```

Log Transform does not work.

Inverse Transform:

```{r}
inv_transform = lm(V8^-1 ~ V9 + I(V12 + V13) + V16, data=data_scaled)
plot(inv_transform, which=1)
```

```{r}
bptest(inv_transform)
```

Inverse is better, so we check to see if inverse squared works.

```{r}
inv_2_transform = lm(V8^-2 ~ V9 + I(V12 + V13) + V16, data=data_scaled)
plot(inv_2_transform, which=1)
```

```{r}
bptest(inv_2_transform)
```

Inverse squared gives us a desirable p-value.

Let's try refitting our model based on an inverse-squared transformation.

```{r}
model.const_variance = lm(V8^-2 ~ V9 + I(V12 + V13) + V16, data_scaled)
summary(model.const_variance)
```

```{r}
summary(lm(V8^-2 ~ ., data_scaled))
```

A low R-squared value is not good in this instance. This shows that transformations will not fix our variance.

## Normality Assumption

```{r}
hist(model$residuals,breaks=100, col=2)
```

```{r}
plot(model, which=2, col=2)
```

```{r}
ks.test(data$V8, "pnorm")
```

The tests we run for the normality assumption reveal our data to not be a normal distribution.

## Checking For Serial Dependence

```{r}
dwtest(model)
```

## Box Cox

```{r}
library(MASS)

bc = boxcox(1/data_scaled$V8 ~ ., data=data_scaled)
lambda = bc$x[which.max(bc$y)]
```

```{r}
model.tr = lm(((V8^lambda-1)/lambda) ~ ., data_scaled)
summary(model.tr)
```

```{r}
plot(model.tr, col=2)
```

```{r}
bptest(model.tr)
```

The box-coc test still isn't the best because R\^2 is not very high, which means that not enough variance can be explained.

# Unusual Observations

## High-Leverage Points

```{r}
p = length(variable.names(model))
n = nrow(data_scaled)
data.leverages = influence(model)$hat
```

```{r}
library(faraway)
halfnorm(data.leverages, nlab=6, labs=as.character(1:length(data.leverages)), ylab="Leverages")
```

1, 2, 123 are noticeably high leverage points on the graph

Let's find the high leverage points using the 2p/n rule.

```{r}
data.leverages.high = data.leverages[data.leverages > (2*p)/n]
data.leverages.high = sort(abs(data.leverages.high), decreasing = TRUE)
head(data.leverages.high)
```

Let's find all the rows that are "bad" high-leverage points

```{r}
# Calculate the IQR for the dependent variable 
IQR_y = IQR(data_scaled$V8)

#Define a range with its lower limit being (Q1 - IQR) and upper limit being (Q3 + IQR) 
QT1_y = quantile(data_scaled$V8, 0.25)
QT3_y = quantile(data_scaled$V8, 0.75)

lower_lim_y = QT1_y - IQR_y
upper_lim_y = QT3_y + IQR_y

vector_lim_y = c(lower_lim_y,upper_lim_y)

# Range for y variable 
vector_lim_y
```

```{r}
# Extract observations with high leverage points from the original data frame 
data.highlev = data_scaled[data.leverages > 2*p/n,]

# Select only the observations with leverage points outside the range 
data.highlev_lower = data.highlev[data.highlev$V8 < vector_lim_y[1], ]
data.highlev_upper = data.highlev[data.highlev$V8 > vector_lim_y[2], ]
data.highlev2 = rbind(data.highlev_lower, data.highlev_upper)
data.highlev2
```

What if we remove the high leverage points?

```{r}
data_scaled.no_bad_highlev = data_scaled[-as.integer(rownames(data.highlev2)),]
model.full.no_bad_highlev = lm(V8 ~ . ,data_scaled.no_bad_highlev)
summary(model.full.no_bad_highlev)
```

```{r}
### Backward Selection Algorithm :) ###
data_to_use = data_scaled.no_bad_highlev
predictors = names(data_to_use)
predictors = predictors[predictors != "V8"]
model_full = lm(reformulate(predictors, "V8"), data=data_to_use)

model_curr = model_full
for (i in 1:(ncol(data_scaled) - 1)) { 
  p_vals = summary(model_curr)$coef[-1,'Pr(>|t|)']
  lowest_predictor = names(p_vals[which.max(p_vals)])
  
  if (p_vals[lowest_predictor] < .05) { break }
  
  reduced_model = lm(reformulate(predictors[predictors != lowest_predictor], "V8"), 
                     data=data_to_use)
  
  if (anova(model_full, reduced_model)$"Pr(>F)"[2] < .05){ break }
  
  predictors = predictors[predictors != lowest_predictor]
  model_curr = reduced_model
}

summary(model_curr)
```

```{r}
plot(model_curr)
```

## Outliers

> Let's find the outliers in our dataset.

```{r}
p = length(variable.names(model))
n = nrow(data_scaled)
```

```{r}
data.resid = rstudent(model)
data.resid.sorted = sort(abs(data.resid), decreasing=TRUE)
head(data.resid.sorted)
```

```{r}
bonferroni_cv = qt(.05/(2*n), n-p-1) 
bonferroni_cv
```

```{r}
data.outliers = data.resid[abs(data.resid) > abs(bonferroni_cv)]
data.outliers
```

We found 3 outliers, represented above.

```{r}
cat("Mean: ", mean(data_scaled$V8))
cat("\nOutlier Values: ", data_scaled$V8[as.integer(names(data.outliers))])
```

What happens if we remove the outliers from our data?

```{r}
data_scaled.no_outliers = data_scaled[-as.integer(names(data.outliers)),]

model.full.no_outliers = lm(V8~ ., data_scaled.no_outliers)
summary(model.full.no_outliers)
```

```{r}
### Backward Selection Algorithm :) ###
data_to_use = data_scaled.no_outliers
predictors = names(data_to_use)
predictors = predictors[predictors != "V8"]
model_full = lm(reformulate(predictors, "V8"), data=data_to_use)

model_curr = model_full
for (i in 1:(ncol(data_scaled) - 1)) { 
  p_vals = summary(model_curr)$coef[-1,'Pr(>|t|)']
  lowest_predictor = names(p_vals[which.max(p_vals)])
  
  if (p_vals[lowest_predictor] < .05) { break }
  
  reduced_model = lm(reformulate(predictors[predictors != lowest_predictor], "V8"), 
                     data=data_to_use)
  
  if (anova(model_full, reduced_model)$"Pr(>F)"[2] < .05){ break }
  
  predictors = predictors[predictors != lowest_predictor]
  model_curr = reduced_model
}

summary(model_curr)
```

```{r}
plot(model_curr)
```

## Influential Points

> Let's find the influential points using Cook's distance

```{r}
data.cooks = cooks.distance(model)
data.cooks[data.cooks >= 1]
```

We have 0 influential points from the cooks distance, where an influential point is a point with a cooks distance greater than or equal to 1.
