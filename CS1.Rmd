---
title: "Case Study 1"
output: html_document
date: "2022-10-10"
---

# Getting Started

> The goal in the case study is to propose a *regression model for predicting the number of practicing physicians* by county using information from the years 1990 and 1992. The data set provides selected county demographic information (CDI) for 440 of the most populous counties in the United States. Each line of data set has an identification number with a county name and state abbreviation and provides information on 14 variables for a single county. The variables are (in the order they are recorded in the `.txt` file)

+-----------------+-----------------------------------+----------------------------------------------------------------------------------------------------+
| Variable Number | Variable Name                     | Variable Description                                                                               |
+=================+===================================+====================================================================================================+
| 1               | Identification Number             | 1-440\                                                                                             |
+-----------------+-----------------------------------+----------------------------------------------------------------------------------------------------+
| 2               | County                            | County Name                                                                                        |
+-----------------+-----------------------------------+----------------------------------------------------------------------------------------------------+
| 3               | State                             | Two-letter state abbreviation                                                                      |
+-----------------+-----------------------------------+----------------------------------------------------------------------------------------------------+
| 4               | Land Area                         | Land area (square miles)                                                                           |
+-----------------+-----------------------------------+----------------------------------------------------------------------------------------------------+
| 5               | Total Population                  | Estimated 1990 population                                                                          |
+-----------------+-----------------------------------+----------------------------------------------------------------------------------------------------+
| 6               | Percent of Population ages 18-24  | Percent of 1990 CDI population                                                                     |
+-----------------+-----------------------------------+----------------------------------------------------------------------------------------------------+
| 7               | Percent of Population 65 or older | Percent of 1990 CDI population ages 65 or older                                                    |
+-----------------+-----------------------------------+----------------------------------------------------------------------------------------------------+
| 8               | Number of Active Physicians       | Number of professional active non-federal physicians during 1990                                   |
+-----------------+-----------------------------------+----------------------------------------------------------------------------------------------------+
| 9               | Number of Hospital Beds           | Total number of beds, cribs, and bassinets during 1990                                             |
+-----------------+-----------------------------------+----------------------------------------------------------------------------------------------------+
| 10              | Total Serious Crimes              | Total number of serious crimes in 1990 as reported by law enforcement agencies                     |
+-----------------+-----------------------------------+----------------------------------------------------------------------------------------------------+
| 11              | Percent High School Graduates     | Percent of adult population who completed 12 or more years of school                               |
+-----------------+-----------------------------------+----------------------------------------------------------------------------------------------------+
| 12              | Percent of Bachelor's Degrees     | Percent of adult population with bachelor's degrees                                                |
+-----------------+-----------------------------------+----------------------------------------------------------------------------------------------------+
| 13              | Percent Below Poverty Level       | Percent of 1990 CDI population with income below poverty level                                     |
+-----------------+-----------------------------------+----------------------------------------------------------------------------------------------------+
| 14              | Percent Unemployment              | Percent of 1990 CDI labor force that is unemployed                                                 |
+-----------------+-----------------------------------+----------------------------------------------------------------------------------------------------+
| 15              | Per Capita Income                 | Per capita income of 1990 CDI population (dollars)                                                 |
+-----------------+-----------------------------------+----------------------------------------------------------------------------------------------------+
| 16              | Total Personal Income             | Total person income of 1990 CDI population (in millions of dollars)                                |
+-----------------+-----------------------------------+----------------------------------------------------------------------------------------------------+
| 17              | Geographic Region                 | Geographic region classification that is used in the US Bureau of the Census: 1=NE, 2=NC, 3=S, 4=W |
+-----------------+-----------------------------------+----------------------------------------------------------------------------------------------------+

## Setting up Data

Importing our data in R

```{r}
data = read.table("data.txt")
head(data)
```

Removing the **ID column** and the categorical **state** and **county** columns

```{r}
data = data[,-c(1, 2, 3)]
head(data2)
```

## Exploring the Data

> Making da graphs

```{r}
hist(data$V8,main="Histogram of V8", breaks=250, col=2)
```

```{r}
hist(data$V8,breaks=250, main="Histogram of V8 < 5000", xlim=range(10, 5000), col=2)
```

```{r}
library(ggplot2)
ggplot(data2, aes(V8)) +
  stat_ecdf(geom = "step", pad=FALSE, color=2) +
  ggtitle("Empircal Distribution of V8")
```

Summary Stats for Each Column

```{r}
summary(data[,1:5])
summary(data[,6:10])
summary(data[,11:12])
```

Correlation Matrix (fancy colors go brrrrr)

```{r}
library(corrplot)
M = cor(data[,-c(1, 2, 3)])
corrplot(M, tl.srt = 45)
```

## Scaling the Data :) (Are you happy now Abhi)

```{r}
data_scaled = data
data_scaled$V9 = data_scaled$V9 / data_scaled$V5
data_scaled$V10 = data_scaled$V10 / data_scaled$V5
```

```{R}
round(cor(data_scaled[,-8]), dig=2)
```

```{r}
M = cor(data_scaled[,-8])
corrplot(M)
```

V5 looks highly correlated with V16, and let's drop it

```{r}
anova(lm(V8~.,data_scaled), lm(V8~.-V5, data_scaled))
```

```{r}
data_scaled = data_scaled[colnames(data_scaled) != "V5"]
```

# MLR Time

> Let's start with a full model

```{r}
model.full = lm(V8 ~ ., data)
summary(model.full)
```

```{r}
plot(model.full)
```

Let's filter out predictors that have high p-values

Permutation Test

```{r}
### Permutation Test Function ###
permutation_test = function(using, to_remove, dataset) {
  n.iter = 1000
  fstats = numeric(n.iter)
  predictors = union(using, to_remove)
  full_model = lm(reformulate(predictors, "V8"), data=dataset)
  for (i in 1:n.iter) {
    newdata = dataset;
    newdata[,to_remove] = newdata[sample(nrow(newdata)), to_remove]
    model = lm(reformulate(predictors, "V8"), data=newdata)
    fstats[i] = summary(model)$fstat[1]
  }
  return (length(fstats[fstats > summary(full_model)$fstat[1]])/n.iter)
}

preds = colnames(data2)
preds = preds[preds != 'V8']
permutation_test(preds, c("V4","V6", "V7", "V10", "V13", "V14", "V17"), data2)
```

```{r}
permutation_test(preds, c("V9"), data2)
```

Backward Selection Time :)

```{r}
### Backward Selection Algorithm :) ###
data_to_use = data_scaled
predictors = names(data_to_use)
predictors = predictors[predictors != "V8"]
all_preds = predictors
model_full = lm(reformulate(predictors, "V8"), data=data_to_use)

model_curr = model_full
for (i in 1:(ncol(data2) - 1)) { 
  p_vals = summary(model_curr)$coef[-1,'Pr(>|t|)']
  lowest_predictor = names(p_vals[which.max(p_vals)])
  # checking p value
  if (p_vals[lowest_predictor] < .05) { break }
  
  reduced_model = lm(reformulate(predictors[predictors != lowest_predictor], "V8"), 
                     data=data_to_use)
  # anova against full model (is adequate replacement)
  if (anova(model_full, reduced_model)$"Pr(>F)"[2] < .05){ break }
  
  # Double checking with permutation test (in case not normal)
  predictors_ = predictors[predictors != lowest_predictor]
  p = permutation_test(all_preds, all_preds[!(all_preds %in% predictors_) ], data_to_use)
  if (p < .05) { break }
  
  predictors = predictors_
  model_curr = reduced_model
}

summary(model_curr)
```

How is it good enough?

```{r}
anova(lm(V8~., data_scaled), lm(V8 ~ V9 + V12 + V13 + V16, data_scaled))
```

nice

Let's see if we can combine V12 and V13

```{r}
anova(lm(V8~., data_scaled), lm(V8 ~ V9 + I(V12 + V13) + V16, data_scaled))
```

woohoo

```{r}
model = lm(V8 ~ V9 + I(V12 + V13) + V16, data_scaled)
summary(model)
```

**We also consistently have high R squared values :)**

# Model Assumptions

## Constant Variance Assumption

```{r}
plot(model, which=1)
```

```{r}
library(lmtest)
bptest(model.reduced)
```

**yikes**

Square Root Transform?

```{r}
sqrt_transform = lm(sqrt(V8) ~ V9 + I(V12 + V13) + V16, data=data2)
plot(sqrt_transform, which=1)
```

```{r}
bptest(sqrt_transform)
```

:(

Log Transform?

```{r}
log_transform = lm(log(V8) ~ V9 + I(V12 + V13) + V16, data=data2)
plot(log_transform, which=1)
```

```{r}
bptest(log_transform)
```

:(

Inverse Transform?

```{r}
inv_transform = lm(V8^-1 ~ V9 + I(V12 + V13) + V16, data=data2)
plot(inv_transform, which=1)
```

```{r}
bptest(inv_transform)
```

Better...

```{r}
inv_2_transform = lm(V8^-2 ~ V9 + I(V12 + V13) + V16, data=data2)
plot(inv_2_transform, which=1)
```

```{r}
bptest(inv_2_transform)
```

YAY WE GOT A GOOD P VALUE

Let's try refitting our model

```{r}
model.const_variance = lm(V8^-2 ~ V9 + I(V12 + V13) + V16, data_scaled)
summary(model.const_variance)
```

```{r}
summary(lm(V8^-2 ~ ., data_scaled))
```

Yikes, a pretty bad R-squared value

-   Variance transforms does not fix this issue

## Normality Assumption

```{r}
hist(model$residuals,breaks=100, col=2)
```

```{r}
plot(model, which=2, col=2)
```

```{r}
ks.test(data2$V8, "pnorm")
```

\^ This is mad because an actual continuous distribution shouldn't have repeat points)

**TLDR: So not normal :(**

## Checking For Serial Dependence

```{r}
dwtest(model)
```

## Box Cox

```{r}
library(MASS)

bc = boxcox(1/data2$V8 ~ ., data=data2)
lambda = bc$x[which.max(bc$y)]
```

```{r}
model.tr = lm(((V8^lambda-1)/lambda) ~ ., data_scaled)
summary(model.tr)
```

```{r}
plot(model.tr, col=2)
```

```{r}
bptest(model.tr)
```

Still bad because R\^2 is not very high (not enough variance explained)

# Unusual Observations

## High-Leverage Points

```{r}
p = length(variable.names(model))
n = nrow(data_scaled)
data.leverages = influence(model)$hat
```

```{r}
library(faraway)
halfnorm(data.leverages, nlab=6, labs=as.character(1:length(data.leverages)), ylab="Leverages")
```

1, 2, 123 are kinda out there :0

Let's find the high leverage points

```{r}
data.leverages.high = data.leverages[data.leverages > (2*p)/n]
data.leverages.high = sort(abs(data.leverages.high), decreasing = TRUE)
head(data.leverages.high)
```

Let's find all the rows that are "bad" high-leverage points

```{r}
# Calculate the IQR for the dependent variable 
IQR_y = IQR(data_scaled$V8)

#Define a range with its lower limit being (Q1 - IQR) and upper limit being (Q3 + IQR) 
QT1_y = quantile(data_scaled$V8, 0.25)
QT3_y = quantile(data_scaled$V8, 0.75)

lower_lim_y = QT1_y - IQR_y
upper_lim_y = QT3_y + IQR_y

vector_lim_y = c(lower_lim_y,upper_lim_y)

# Range for y variable 
vector_lim_y
```

```{r}
# Extract observations with high leverage points from the original data frame 
data.highlev = data_scaled[data.leverages > 2*p/n,]

# Select only the observations with leverage points outside the range 
data.highlev_lower = data.highlev[data.highlev$V8 < vector_lim_y[1], ]
data.highlev_upper = data.highlev[data.highlev$V8 > vector_lim_y[2], ]
data.highlev2 = rbind(data.highlev_lower, data.highlev_upper)
data.highlev2
```

What is we remove the high leverage points?

```{r}
data_scaled.no_bad_highlev = data_scaled[-as.integer(rownames(data.highlev2)),]
model.full.no_bad_highlev = lm(V8 ~ . ,data_scaled.no_bad_highlev)
summary(model.full.no_bad_highlev)
```

```{r}
### Backward Selection Algorithm :) ###
data_to_use = data_scaled.no_bad_highlev
predictors = names(data_to_use)
predictors = predictors[predictors != "V8"]
model_full = lm(reformulate(predictors, "V8"), data=data_to_use)

model_curr = model_full
for (i in 1:(ncol(data2) - 1)) { 
  p_vals = summary(model_curr)$coef[-1,'Pr(>|t|)']
  lowest_predictor = names(p_vals[which.max(p_vals)])
  
  if (p_vals[lowest_predictor] < .05) { break }
  
  reduced_model = lm(reformulate(predictors[predictors != lowest_predictor], "V8"), 
                     data=data_to_use)
  
  if (anova(model_full, reduced_model)$"Pr(>F)"[2] < .05){ break }
  
  predictors = predictors[predictors != lowest_predictor]
  model_curr = reduced_model
}

summary(model_curr)
```

```{r}
plot(model_curr)
```

## Outliers

> Let's find the outliers in our dataset

```{r}
p = length(variable.names(model))
n = nrow(data_scaled)
```

```{r}
data.resid = rstudent(model)
data.resid.sorted = sort(abs(data.resid), decreasing=TRUE)
head(data.resid.sorted)
```

```{r}
bonferroni_cv = qt(.05/(2*n), n-p-1) 
bonferroni_cv
```

```{r}
data.outliers = data.resid[abs(data.resid) > abs(bonferroni_cv)]
data.outliers
```

We got a couple of outliers over here :)

```{r}
cat("Mean: ", mean(data2$V8))
cat("\nOutlier Values: ", data2$V8[as.integer(names(data.outliers))])
```

What happens if we remove them from our data?

```{r}
data_scaled.no_outliers = data_scaled[-as.integer(names(data.outliers)),]

model.full.no_outliers = lm(V8~ ., data_scaled.no_outliers)
summary(model.full.no_outliers)
```

```{r}
### Backward Selection Algorithm :) ###
data_to_use = data_scaled.no_outliers
predictors = names(data_to_use)
predictors = predictors[predictors != "V8"]
model_full = lm(reformulate(predictors, "V8"), data=data_to_use)

model_curr = model_full
for (i in 1:(ncol(data2) - 1)) { 
  p_vals = summary(model_curr)$coef[-1,'Pr(>|t|)']
  lowest_predictor = names(p_vals[which.max(p_vals)])
  
  if (p_vals[lowest_predictor] < .05) { break }
  
  reduced_model = lm(reformulate(predictors[predictors != lowest_predictor], "V8"), 
                     data=data_to_use)
  
  if (anova(model_full, reduced_model)$"Pr(>F)"[2] < .05){ break }
  
  predictors = predictors[predictors != lowest_predictor]
  model_curr = reduced_model
}

summary(model_curr)
```

```{r}
plot(model_curr)
```

## Influential Points

> Let's find the influential points using Cook's distance

```{r}
data.cooks = cooks.distance(model)
data.cooks[data.cooks >= 1]
```
